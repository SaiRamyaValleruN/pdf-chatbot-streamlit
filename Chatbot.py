import streamlit as st
from PyPDF2 import PdfReader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS
from transformers import pipeline

# Set page config
st.set_page_config(page_title="Free PDF Q&A Chatbot", layout="wide")
st.title("üìÑ Free PDF Q&A Chatbot ")

# Sidebar upload
with st.sidebar:
    st.header("üìé Upload your PDF")
    uploaded_pdf = st.file_uploader("Choose a PDF file", type="pdf")

if uploaded_pdf:
    pdf_reader = PdfReader(uploaded_pdf)
    text = ""
    for page in pdf_reader.pages:
        page_text = page.extract_text()
        if page_text:
            text += page_text

    if not text.strip():
        st.warning("‚ö†Ô∏è Could not extract any text from the PDF. Try another one.")
    else:
        # Split into chunks
        splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)
        chunks = splitter.split_text(text)

        if len(chunks) == 0:
            st.warning("‚ö†Ô∏è No valid text chunks to embed.")
        else:
            # Create embeddings
            embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
            try:
                vector_store = FAISS.from_texts(chunks, embeddings)

                # ‚úÖ Use MMR retriever for better coverage
                retriever = vector_store.as_retriever(
                    search_type="mmr", search_kwargs={"k": 20}
                )

                st.success("‚úÖ PDF processed successfully. You can now ask questions!")

                # ‚úÖ Extractive QA model
                qa_pipeline = pipeline("question-answering", model="deepset/roberta-base-squad2")

                def answer_question(query):
                    # Retrieve candidate chunks
                    docs = retriever.get_relevant_documents(query)

                    if not docs:
                        return "‚ùå No relevant context found in the PDF.", None

                    best_answer = None
                    best_score = -1
                    best_doc = None

                    for doc in docs:
                        result = qa_pipeline(question=query, context=doc.page_content)

                        # ‚úÖ Always keep the best scoring answer
                        if result["score"] > best_score:
                            best_answer = result["answer"]
                            best_score = result["score"]
                            best_doc = doc.page_content

                    if not best_answer or best_answer.strip() == "":
                        # Fallback: show nearest context
                        return "‚ö†Ô∏è No clear answer, but here‚Äôs something related:", docs[0].page_content[:400]

                    # ‚úÖ Return even low-confidence answers
                    return f"{best_answer} (nearest match, confidence: {round(best_score, 2)})", best_doc

                # Ask questions
                query = st.text_input("‚ùì Ask a question about the PDF:")
                if query:
                    answer, context = answer_question(query)
                    st.subheader("ü§ñ Answer:")
                    st.success(answer)

                    # ‚úÖ Show context if available
                    if context:
                        with st.expander("üìÑ Show source context from PDF"):
                            st.write(context)

            except Exception as e:
                st.error(f"Something went wrong while embedding: {e}")
